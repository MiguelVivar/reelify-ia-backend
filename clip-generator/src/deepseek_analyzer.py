import os
import uuid
import logging
import asyncio
import aiohttp
import whisper
import ffmpeg
import subprocess
import json
import numpy as np
import librosa
import re
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass
from collections import defaultdict
from config import settings

logger = logging.getLogger(__name__)

@dataclass
class ClipCandidate:
    """Estructura para candidatos de clips con metadatos avanzados"""
    start: float
    end: float
    base_score: float
    emotional_intensity: float
    speech_clarity: float
    keyword_density: float
    conversation_flow: float
    audio_energy: float
    final_score: float
    reason: str
    transcription: str
    confidence: float

class ViralContentDetector:
    """Detector avanzado de contenido viral con an√°lisis sem√°ntico y temporal"""
    
    def __init__(self):
        # Patrones virales por categor√≠a con pesos din√°micos
        self.viral_patterns = {
            'emociones_fuertes': {
                'patterns': [
                    r'\b(incre√≠ble|impresionante|alucinante|brutal|√©pico)\b',
                    r'\b(no puedo creer|no way|imposible|qu√© locura)\b',
                    r'\b(amor|odio|detesto|adoro|fascina)\b',
                    r'\b(perfecto|horrible|terrible|maravilloso)\b'
                ],
                'weight': 2.5
            },
            'reacciones_autenticas': {
                'patterns': [
                    r'\b(wow|guau|ostras|joder|madre m√≠a)\b',
                    r'\b(en serio|de verdad|no me digas|qu√© fuerte)\b',
                    r'\b(me muero|me parto|me cago)\b',
                    r'[!]{2,}|[?]{2,}'
                ],
                'weight': 2.0
            },
            'humor_engagement': {
                'patterns': [
                    r'\b(gracioso|divertido|chistoso|c√≥mico)\b',
                    r'\b(jajaja|jejeje|jijijij)\b',
                    r'\b(meme|viral|tendencia|trend)\b',
                    r'\b(risa|re√≠r|carcajada)\b'
                ],
                'weight': 1.8
            },
            'contenido_controversial': {
                'patterns': [
                    r'\b(pol√©mico|controversial|esc√°ndalo)\b',
                    r'\b(opini√≥n|debate|discusi√≥n|problema)\b',
                    r'\b(critica|defiende|ataca|pol√©mica)\b'
                ],
                'weight': 1.5
            },
            'urgencia_accion': {
                'patterns': [
                    r'\b(ahora|inmediatamente|urgente|r√°pido)\b',
                    r'\b(limitado|exclusivo|por tiempo limitado)\b',
                    r'\b(√∫ltima oportunidad|no te pierdas)\b'
                ],
                'weight': 1.3
            },
            'valor_informativo': {
                'patterns': [
                    r'\b(secreto|truco|tip|consejo|hack)\b',
                    r'\b(aprende|descubre|revela|desvela)\b',
                    r'\b(m√©todo|t√©cnica|estrategia|f√≥rmula)\b'
                ],
                'weight': 1.2
            }
        }
        
        # Patrones anti-virales (reducen score)
        self.anti_viral_patterns = [
            r'\b(aburrido|mon√≥tono|lento|pesado)\b',
            r'\b(complicado|dif√≠cil|complejo|t√©cnico)\b',
            r'\b(largo|extenso|detallado|exhaustivo)\b',
            r'\b(obvio|evidente|normal|t√≠pico)\b'
        ]

class DeepseekVideoAnalyzer:
    """
    Analizador de video que usa Deepseek de OpenRouter para identificar 
    los mejores momentos del video antes de generar clips.
    """
    
    def __init__(self):
        self.api_key = settings.openrouter_api_key
        self.base_url = settings.openrouter_base_url
        self.model = settings.deepseek_model
        self.temp_dir = settings.temp_dir
        
        # Configuraci√≥n de an√°lisis mejorada
        self.segment_duration = settings.analysis_segment_duration
        self.max_segments = settings.max_analysis_segments
        self.highlight_threshold = settings.highlight_threshold
        
        # Inicializar detector de contenido viral
        self.viral_detector = ViralContentDetector()
        
        # Configuraci√≥n de calidad temporal din√°mica
        self.min_clip_separation = settings.min_clip_separation_seconds  # Usar configuraci√≥n
        self.optimal_clip_duration = (settings.optimal_viral_duration_min, settings.optimal_viral_duration_max)  # Rango √≥ptimo din√°mico
        self.max_clips_per_video = settings.max_clips_per_video  # M√°ximo din√°mico
        self.absolute_min_duration = settings.absolute_min_clip_duration  # M√≠nimo absoluto
        self.absolute_max_duration = settings.absolute_max_clip_duration  # M√°ximo absoluto
        
        # Inicializar Whisper para transcripciones
        # No cargar Whisper autom√°ticamente: usar carga perezosa en _transcribe_segment
        self.whisper_model = None
        
        os.makedirs(self.temp_dir, exist_ok=True)

    def _analyze_viral_content(self, text: str) -> Dict[str, float]:
        """An√°lisis avanzado de contenido viral con puntuaci√≥n detallada"""
        if not text:
            return {'score': 0.0, 'confidence': 0.0, 'category_scores': {}}
        
        text_lower = text.lower()
        category_scores = {}
        total_weight = 0
        weighted_score = 0
        
        # Analizar cada categor√≠a de contenido viral
        for category, config in self.viral_detector.viral_patterns.items():
            category_score = 0
            matches = 0
            
            for pattern in config['patterns']:
                pattern_matches = len(re.findall(pattern, text_lower))
                if pattern_matches > 0:
                    matches += pattern_matches
                    category_score += pattern_matches
            
            # Normalizar score de categor√≠a
            if matches > 0:
                # Bonus por diversidad de patrones en la categor√≠a
                pattern_diversity = len([p for p in config['patterns'] if re.search(p, text_lower)]) / len(config['patterns'])
                category_score = min(category_score * (1 + pattern_diversity), 5.0)
            
            category_scores[category] = category_score
            weighted_score += category_score * config['weight']
            total_weight += config['weight']
        
        # Aplicar penalizaciones por contenido anti-viral
        penalty = 0
        for anti_pattern in self.viral_detector.anti_viral_patterns:
            penalty += len(re.findall(anti_pattern, text_lower)) * 0.3
        
        # Calcular score final
        if total_weight > 0:
            base_score = weighted_score / total_weight
        else:
            base_score = 0
        
        final_score = max(0, base_score - penalty)
        
        # Calcular confianza basada en la cantidad de evidencia
        total_matches = sum(category_scores.values())
        confidence = min(total_matches / 3.0, 1.0)  # Confianza m√°xima con 3+ matches
        
        return {
            'score': final_score,
            'confidence': confidence,
            'category_scores': category_scores,
            'penalty': penalty
        }

    def _analyze_speech_clarity(self, transcription: str, segment_duration: float) -> float:
        """Analiza la claridad del discurso basada en la transcripci√≥n"""
        if not transcription or segment_duration <= 0:
            return 0.0
        
        words = transcription.split()
        word_count = len(words)
        
        if word_count == 0:
            return 0.0
        
        # Calcular palabras por segundo
        words_per_second = word_count / segment_duration
        
        # Rango √≥ptimo de palabras por segundo para claridad
        optimal_wps_range = (2.0, 4.0)
        
        if optimal_wps_range[0] <= words_per_second <= optimal_wps_range[1]:
            clarity_score = 1.0
        elif words_per_second < optimal_wps_range[0]:
            # Demasiado lento
            clarity_score = words_per_second / optimal_wps_range[0]
        else:
            # Demasiado r√°pido
            clarity_score = optimal_wps_range[1] / words_per_second
        return float(max(0.0, min(1.0, clarity_score)))

    def _compute_candidate_duration(self, candidate: ClipCandidate, video_duration: Optional[float] = None) -> float:
        """Calcula una duraci√≥n objetivo para un candidato combinando:
        - Duraci√≥n sugerida por Deepseek (si est√° en la raz√≥n o metadata)
        - Densidad de palabras (words/sec) para evitar clips demasiado largos o cortos
        - Jitter determinista por √≠ndice para evitar duraciones id√©nticas
        - Respeta l√≠mites absolutos y rango √≥ptimo
        """
        # Preferencia por duraci√≥n √≥ptima del sistema (rango)
        min_opt, max_opt = self.optimal_clip_duration

        # Intentar extraer sugerencia de duraci√≥n del reason o metadata
        suggested = None
        try:
            # Buscar patrones tipo 'duraci√≥n: 12s' o 'optimal_duration' si viene en metadata
            m = re.search(r'(\b\d+(?:\.\d+)?)(?:s|sec|secs)?', candidate.reason or '')
            if m:
                suggested = float(m.group(1))
        except Exception:
            suggested = None

        # Calcular words/sec a partir de la transcripci√≥n
        words = (candidate.transcription or '').split()
        word_count = max(0, len(words))
        cand_duration_est = (candidate.end - candidate.start) if (candidate.end > candidate.start) else None

        words_per_second = None
        if cand_duration_est and cand_duration_est > 0:
            words_per_second = word_count / cand_duration_est

        # Base duration heuristic
        if suggested:
            target = suggested
        elif words_per_second and words_per_second > 0:
            # Ajustar la duraci√≥n para que la densidad caiga en un rango √≥ptimo (2-4 wps)
            optimal_wps = 3.0
            target = max(min_opt, min(max_opt, (word_count / optimal_wps) if word_count > 0 else min_opt))
        elif cand_duration_est:
            target = cand_duration_est
        else:
            target = (min_opt + max_opt) / 2.0

        # Aplicar jitter determinista basado en start para reproducibilidad
        jitter = (hash((round(candidate.start, 3), round(candidate.end, 3))) % 11 - 5) / 100.0  # +-0.05 ajuste
        target = target * (1.0 + jitter)

        # Clamp final
        target = max(self.absolute_min_duration, min(self.absolute_max_duration, target))

        # No exceder duraci√≥n total del video
        if video_duration:
            target = min(target, video_duration)

        return float(target)
        
        # Bonus por diversidad de vocabulario
        unique_words = len(set(word.lower() for word in words))
        vocabulary_diversity = unique_words / word_count if word_count > 0 else 0
        
        # Ajustar por diversidad (m√°ximo 20% de bonus)
        final_score = min(clarity_score * (1 + vocabulary_diversity * 0.2), 1.0)
        
        return final_score

    def _analyze_conversation_flow(self, transcription: str) -> float:
        """Analiza el flujo conversacional para engagement"""
        if not transcription:
            return 0.0
        
        text_lower = transcription.lower()
        flow_score = 0.0
        
        # Patrones de buen flujo conversacional
        flow_patterns = [
            r'\b(pero|sin embargo|aunque|adem√°s|tambi√©n)\b',  # Conectores
            r'\b(entonces|por eso|as√≠ que|por tanto)\b',  # Causa-efecto
            r'\b(primero|segundo|despu√©s|finalmente)\b',  # Secuencia
            r'\b(por ejemplo|es decir|o sea|vamos)\b',  # Explicaci√≥n
            r'[?]',  # Preguntas (engagement)
            r'\b(mira|f√≠jate|imag√≠nate|piensa)\b'  # Llamadas de atenci√≥n
        ]
        
        pattern_count = 0
        for pattern in flow_patterns:
            matches = len(re.findall(pattern, text_lower))
            pattern_count += matches
        
        # Normalizar por longitud del texto
        words = len(transcription.split())
        if words > 0:
            flow_density = pattern_count / words
            flow_score = min(flow_density * 20, 1.0)  # Escalar apropiadamente
        
        return flow_score

    def _calculate_advanced_score(self, clip_candidate: ClipCandidate) -> float:
        """Calcula puntuaci√≥n final avanzada con m√∫ltiples factores"""
        
        # Pesos para cada factor
        weights = {
            'base_score': 0.35,
            'emotional_intensity': 0.25,
            'speech_clarity': 0.15,
            'conversation_flow': 0.15,
            'duration_optimality': 0.10
        }
        
        # Calcular optimalidad de duraci√≥n
        duration = clip_candidate.end - clip_candidate.start
        optimal_min, optimal_max = self.optimal_clip_duration
        
        if optimal_min <= duration <= optimal_max:
            duration_score = 1.0
        elif duration < optimal_min:
            duration_score = duration / optimal_min
        else:
            duration_score = optimal_max / duration
        
        # Combinar todos los factores
        final_score = (
            clip_candidate.base_score * weights['base_score'] +
            clip_candidate.emotional_intensity * weights['emotional_intensity'] +
            clip_candidate.speech_clarity * weights['speech_clarity'] +
            clip_candidate.conversation_flow * weights['conversation_flow'] +
            duration_score * weights['duration_optimality']
        )
        
        # Aplicar bonus por confianza alta
        confidence_bonus = 1.0 + (clip_candidate.confidence * 0.2)
        final_score *= confidence_bonus
        
        return min(final_score, 1.0)

    async def analyze_video_highlights_with_metadata(self, video_path: str) -> List[Dict[str, Any]]:
        """
        Analiza todo el video para identificar los mejores momentos para clips.
        Retorna metadatos completos incluyendo scores y razones.
        
        Args:
            video_path: Ruta al archivo de video
            
        Returns:
            Lista de diccionarios con start, end, score, reason
        """
        try:
            if not self.api_key:
                logger.warning("API key de OpenRouter no configurada, usando an√°lisis b√°sico")
                return await self._fallback_analysis_with_metadata(video_path)
            
            logger.info(f"Iniciando an√°lisis de video con Deepseek (con metadatos): {video_path}")
            
            # 1. Obtener duraci√≥n del video
            duration = await self._get_video_duration(video_path)
            if duration <= 0:
                logger.error("No se pudo obtener la duraci√≥n del video")
                return []
            
            logger.info(f"Duraci√≥n del video: {duration:.2f}s")
            
            # 2. Dividir video en segmentos para an√°lisis
            segments = self._create_analysis_segments(duration)
            logger.info(f"Video dividido en {len(segments)} segmentos para an√°lisis")
            
            # 3. Transcribir cada segmento
            segment_transcriptions = []
            for i, (start, end) in enumerate(segments):
                logger.info(f"Transcribiendo segmento {i+1}/{len(segments)}: {start:.1f}s - {end:.1f}s")
                transcription = await self._transcribe_segment(video_path, start, end)
                if transcription:
                    segment_transcriptions.append({
                        'start': start,
                        'end': end,
                        'transcription': transcription,
                        'segment_index': i
                    })
                    logger.info(f"Segmento {i+1} transcrito: {len(transcription)} caracteres")
                else:
                    logger.warning(f"No se pudo transcribir el segmento {i+1}")
            
            if not segment_transcriptions:
                logger.warning("No se pudieron transcribir segmentos, usando an√°lisis de respaldo")
                return await self._fallback_analysis_with_metadata(video_path)
            
            logger.info(f"Total de segmentos transcritos: {len(segment_transcriptions)}")
            
            # 4. Analizar con Deepseek
            highlights = await self._analyze_with_deepseek(segment_transcriptions)
            
            if not highlights:
                logger.warning("Deepseek no devolvi√≥ highlights, usando an√°lisis de respaldo")
                return await self._fallback_analysis_with_metadata(video_path)
            
            # 5. Convertir a clips v√°lidos con metadatos
            valid_clips = self._convert_to_clips_with_metadata(highlights, duration)
            
            logger.info(f"An√°lisis completado: {len(valid_clips)} clips identificados con metadatos")
            return valid_clips
            
        except Exception as e:
            logger.error(f"Error en an√°lisis de video con metadatos: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return await self._fallback_analysis_with_metadata(video_path)

    async def analyze_video_highlights(self, video_path: str) -> List[Tuple[float, float]]:
        """
        Analiza todo el video para identificar los mejores momentos para clips.
        
        Args:
            video_path: Ruta al archivo de video
            
        Returns:
            Lista de tuplas (start_time, end_time) con los mejores momentos
        """
        try:
            if not self.api_key:
                logger.warning("API key de OpenRouter no configurada, usando an√°lisis b√°sico")
                return await self._fallback_analysis(video_path)
            
            logger.info(f"Iniciando an√°lisis de video con Deepseek: {video_path}")
            
            # 1. Obtener duraci√≥n del video
            duration = await self._get_video_duration(video_path)
            if duration <= 0:
                logger.error("No se pudo obtener la duraci√≥n del video")
                return []
            
            # 2. Dividir video en segmentos para an√°lisis
            segments = self._create_analysis_segments(duration)
            logger.info(f"Video dividido en {len(segments)} segmentos para an√°lisis")
            
            # 3. Transcribir cada segmento
            segment_transcriptions = []
            for i, (start, end) in enumerate(segments):
                logger.info(f"Transcribiendo segmento {i+1}/{len(segments)}: {start:.1f}s - {end:.1f}s")
                transcription = await self._transcribe_segment(video_path, start, end)
                if transcription:
                    segment_transcriptions.append({
                        'start': start,
                        'end': end,
                        'transcription': transcription,
                        'segment_index': i
                    })
                    logger.info(f"Segmento {i+1} transcrito: {len(transcription)} caracteres")
                else:
                    logger.warning(f"No se pudo transcribir el segmento {i+1}")

            logger.info(f"Transcripciones totales recogidas: {len(segment_transcriptions)} de {len(segments)} segmentos")
            
            # 4. Analizar con Deepseek
            highlights = await self._analyze_with_deepseek(segment_transcriptions)
            
            # 5. Convertir a clips v√°lidos y filtrar solapamientos
            valid_clips = self._convert_to_clips(highlights)
            
            logger.info(f"An√°lisis completado: {len(valid_clips)} clips identificados")
            return valid_clips
            
        except Exception as e:
            logger.error(f"Error en an√°lisis de video: {e}")
            return await self._fallback_analysis(video_path)
    
    def _create_analysis_segments(self, duration: float) -> List[Tuple[float, float]]:
        """Crea segmentos para an√°lisis del video completo"""
        segments: List[Tuple[float, float]] = []

        # Si se fuerza cobertura completa, generamos segmentos contiguos hasta un tope seguro
        if getattr(settings, 'force_full_coverage', False):
            max_safe_segments = min(self.max_segments, 300)  # tope de seguridad
            estimated_segments = int((duration + self.segment_duration - 1) // self.segment_duration)
            if estimated_segments > max_safe_segments:
                logger.warning(f"FORCE_FULL_COVERAGE activo pero el n√∫mero de segmentos ({estimated_segments}) excede el tope seguro ({max_safe_segments}). Se usar√°n {max_safe_segments} segmentos distribuidos uniformemente.")
                # distribuir max_safe_segments a lo largo del video
                step = duration / max_safe_segments
                for i in range(max_safe_segments):
                    start = max(0.0, i * step)
                    end = min(duration, start + self.segment_duration)
                    if end - start >= 0.01:
                        segments.append((start, end))
                return segments
            # si est√° dentro del tope, hacer segmentos contiguos
            current_time = 0.0
            while current_time < duration and len(segments) < estimated_segments:
                end_time = min(current_time + self.segment_duration, duration)
                segments.append((current_time, end_time))
                current_time += self.segment_duration
            return segments

        # Comportamiento por defecto: si el video cabe en max_segments se hacen contiguos
        if duration <= self.segment_duration * self.max_segments:
            current_time = 0.0
            while current_time < duration and len(segments) < self.max_segments:
                end_time = min(current_time + self.segment_duration, duration)
                segments.append((current_time, end_time))
                current_time += self.segment_duration
            return segments

        # Si el video es mucho m√°s largo que el n√∫mero m√°ximo de segmentos,
        # distribuimos `max_segments` ventanas a lo largo de todo el video para cubrir todas las partes.
        total_slots = self.max_segments
        step = duration / total_slots
        for i in range(total_slots):
            start = max(0.0, i * step)
            end = min(duration, start + self.segment_duration)
            if end - start < 0.01:
                continue
            segments.append((start, end))

        return segments

    def _compute_backup_segment_duration(self, position: float, index: int, total: int, min_d: float, max_d: float) -> float:
        """Calcula una duraci√≥n inteligente para clips de respaldo.

        - `position`: posici√≥n relativa en el video (0..1)
        - `index`: √≠ndice del clip (0..total-1)
        - `total`: n√∫mero total de clips
        - `min_d`, `max_d`: l√≠mites absolutos

        La l√≥gica busca:
        - Clips del principio y final m√°s cortos (gancho / cierre)
        - Clips centrales m√°s largos (m√°s contexto)
        - A√±adir una peque√±a variaci√≥n (jitter) dependiente del √≠ndice para evitar duraciones id√©nticas
        - Respetar `absolute_min_duration` y `absolute_max_duration`
        """
        # Peso base seg√∫n distancia al centro (0..1)
        center_distance = abs(0.5 - position)

        # M√°s cerca del centro -> m√°s largo. Invertir y normalizar.
        center_influence = 1.0 - (center_distance * 2.0)  # 1 en centro, 0 en extremos
        center_influence = max(0.0, min(1.0, center_influence))

        # Base duration interpolada entre min_d y max_d
        base_duration = min_d + (max_d - min_d) * (0.2 + 0.8 * center_influence)

        # Reducir levemente primer/√∫ltimo clip para gancho/cierre
        edge_factor = 1.0
        if index == 0 or index == total - 1:
            edge_factor = 0.65
        elif index == 1 or index == total - 2:
            edge_factor = 0.85

        duration = base_duration * edge_factor

        # A√±adir jitter determin√≠stico peque√±o basado en √≠ndice (para reproducibilidad)
        jitter = (self._deterministic_jitter(index) - 0.5) * 0.15 * duration
        duration += jitter

        # Respetar l√≠mites absolutos configurados
        duration = max(duration, self.absolute_min_duration, min_d)
        duration = min(duration, self.absolute_max_duration, max_d)

        return float(duration)

    def _deterministic_jitter(self, index: int) -> float:
        """Genera un valor pseudoaleatorio determin√≠stico 0..1 a partir del √≠ndice."""
        # Simple LCG para reproducibilidad
        a = 1664525
        c = 1013904223
        m = 2 ** 32
        seed = (index + 1) * 9781
        val = (a * seed + c) % m
        return (val / m)

    def _parse_time_to_seconds(self, value: Any) -> Optional[float]:
        """Parsea distintos formatos de tiempo a segundos.

        Acepta n√∫meros (int/float), cadenas como 'mm:ss' o 'hh:mm:ss' o '123.5'.
        Devuelve None si no puede parsear.
        """
        if value is None:
            return None
        try:
            if isinstance(value, (int, float)):
                return float(value)
            if isinstance(value, str):
                v = value.strip()
                # Formato hh:mm:ss o mm:ss
                parts = v.split(":")
                if len(parts) == 3:
                    h, m, s = parts
                    return float(h) * 3600 + float(m) * 60 + float(s)
                if len(parts) == 2:
                    m, s = parts
                    return float(m) * 60 + float(s)
                # Simple n√∫mero en string
                return float(v)
        except Exception:
            return None

    def _clamp(self, val: float, low: float, high: float) -> float:
        return max(low, min(high, val))

    def _extract_json_from_text(self, text: str) -> Optional[str]:
        """Extrae el primer objeto JSON v√°lido de un texto que puede contener markdown u otros wrappers.

        Devuelve la substring con JSON o None si no encuentra.
        """
        if not text:
            return None
        # Intento simple: buscar primer '{' y √∫ltimo '}'
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1 and end > start:
            candidate = text[start:end+1]
            return candidate

        # Eliminar bloques de c√≥digo Markdown y buscar de nuevo
        cleaned = re.sub(r'```[\s\S]*?```', '', text)
        cleaned = cleaned.strip()
        start = cleaned.find('{')
        end = cleaned.rfind('}')
        if start != -1 and end != -1 and end > start:
            return cleaned[start:end+1]

        return None
    
    async def _transcribe_segment(self, video_path: str, start_time: float, end_time: float) -> Optional[str]:
        """Transcribe un segmento espec√≠fico del video"""
        # Lazy-load modelo Whisper si est√° configurado para cargarse en inicio o si no est√° a√∫n cargado
        if not self.whisper_model:
            try:
                if settings.whisper_load_on_start or True:
                    model_name = getattr(settings, 'whisper_model_name', 'base')
                    logger.info(f"Cargando modelo Whisper '{model_name}' para transcripci√≥n (lazy-load)")
                    self.whisper_model = whisper.load_model(model_name)
                    logger.info("Modelo Whisper cargado correctamente (lazy)")
            except Exception as e:
                logger.error(f"Error cargando modelo Whisper: {e}")
                self.whisper_model = None
        if not self.whisper_model:
            logger.warning("Modelo Whisper no disponible tras intento de carga")
            return None
        
        try:
            # Extraer audio del segmento
            segment_id = str(uuid.uuid4())[:8]
            audio_path = os.path.join(self.temp_dir, f"audio_segment_{segment_id}.wav")
            
            logger.info(f"Extrayendo audio del segmento {start_time:.1f}s - {end_time:.1f}s")
            
            # Usar ffmpeg para extraer el audio del segmento
            try:
                try:
                    (
                        ffmpeg
                        .input(video_path, ss=start_time, t=end_time - start_time)
                        .output(audio_path, acodec='pcm_s16le', ac=1, ar='16000')
                        .overwrite_output()
                        .run(quiet=True)
                    )
                except Exception as e:
                    # Fallback robusto: intentar invocar ffmpeg directamente por subprocess
                    logger.warning(f"ffmpeg-python fall√≥ ({e}), intentando fallback con subprocess")
                    cmd = [
                        'ffmpeg',
                        '-y',
                        '-ss', str(start_time),
                        '-t', str(end_time - start_time),
                        '-i', video_path,
                        '-acodec', 'pcm_s16le',
                        '-ac', '1',
                        '-ar', '16000',
                        audio_path
                    ]
                    try:
                        completed = subprocess.run(cmd, check=True, capture_output=True, timeout=30)
                        if completed.returncode != 0:
                            logger.error(f"FFmpeg returned non-zero code: {completed.returncode} - {completed.stderr.decode(errors='ignore')}" )
                    except subprocess.TimeoutExpired as te:
                        logger.error(f"FFmpeg timeout al extraer segmento: {te}")
                    except subprocess.CalledProcessError as ce:
                        stderr = ce.stderr.decode(errors='ignore') if ce.stderr else str(ce)
                        logger.error(f"FFmpeg error en subprocess: {stderr}")
            except Exception as e:
                logger.error(f"Error extrayendo audio del segmento: {e}")
                return None
            
            # Verificar que el archivo de audio se cre√≥
            if not os.path.exists(audio_path):
                logger.error(f"Archivo de audio no se cre√≥: {audio_path}")
                return None
            
            # Transcribir con Whisper
            try:
                logger.info(f"Transcribiendo audio: {audio_path}")
                result = self.whisper_model.transcribe(audio_path, language='es')  # Especificar espa√±ol
                transcription = result["text"].strip()
                
                # Limpiar archivo temporal
                if os.path.exists(audio_path):
                    os.remove(audio_path)
                
                if transcription:
                    logger.info(f"Transcripci√≥n exitosa: {len(transcription)} caracteres")
                    return transcription
                else:
                    logger.warning("Transcripci√≥n vac√≠a")
                    return None
                
            except Exception as e:
                logger.error(f"Error en transcripci√≥n: {e}")
                if os.path.exists(audio_path):
                    os.remove(audio_path)
                return None
                
        except Exception as e:
            logger.error(f"Error en transcripci√≥n de segmento: {e}")
            return None
    
    async def _analyze_with_deepseek(self, segment_transcriptions: List[Dict]) -> List[Dict]:
        """Analiza las transcripciones con Deepseek para identificar mejores momentos"""
        try:
            # Preparar el prompt para Deepseek
            transcription_text = "\n\n".join([
                f"Segmento {seg['segment_index']} ({seg['start']:.1f}s - {seg['end']:.1f}s):\n{seg['transcription']}"
                for seg in segment_transcriptions if seg['transcription']
            ])
            
            logger.info(f"Enviando {len(segment_transcriptions)} transcripciones a Deepseek para an√°lisis")
            
            prompt = f"""Eres un experto en identificar contenido VIRAL en redes sociales. Analiza estas transcripciones y selecciona TODOS los momentos con potencial viral real.

TRANSCRIPCIONES:
{transcription_text}

CRITERIOS VIRALES FLEXIBLES (Score m√≠nimo 0.65):
üî• EMOCIONES INTENSAS: Reacciones aut√©nticas, sorpresas, risas explosivas
üí¨ FRASES MEMORABLES: Citas pegajosas, declaraciones pol√©micas apropiadas
‚ö° MOMENTOS CLIM√ÅTICOS: Puntos de tensi√≥n, revelaciones, plot twists
üéØ VALOR √öNICO: Informaci√≥n exclusiva, secretos, trucos desconocidos
üí° ENGAGEMENT NATURAL: Preguntas directas, llamadas a la acci√≥n impl√≠citas
üîÑ POTENCIAL SHARE: Contenido que la gente querr√≠a compartir inmediatamente

EVITAR ABSOLUTAMENTE:
‚ùå Explicaciones largas sin gancho emocional
‚ùå Momentos de transici√≥n o relleno sin valor
‚ùå Contenido t√©cnico sin emoci√≥n
‚ùå Repeticiones o informaci√≥n redundante
‚ùå Momentos mon√≥tonos o de baja energ√≠a

REGLAS TEMPORALES DIN√ÅMICAS:
- Duraci√≥n COMPLETAMENTE FLEXIBLE: 15 segundos - 3 minutos seg√∫n el contenido
- Clips cortos (15-30s): Para momentos de m√°ximo impacto, reacciones explosivas
- Clips medianos (30-90s): Para historias completas, explicaciones valiosas
- M√çNIMO 1 MINUTO de separaci√≥n entre clips
- SIN L√çMITE FIJO de clips - encuentra TODOS los momentos virales
- La duraci√≥n debe ajustarse PERFECTAMENTE al contenido
- Priorizar CALIDAD y COMPLETITUD sobre restricciones arbitrarias

DISTRIBUCI√ìN INTELIGENTE:
- Analiza TODO el video completo
- Identifica CADA momento con potencial viral
- No te limites a un n√∫mero espec√≠fico de clips
- Distribuye clips a lo largo del video seg√∫n el contenido
- Si hay 10 momentos virales, devuelve 10 clips
- Si hay 2 momentos virales, devuelve 2 clips

INSTRUCCIONES ESPEC√çFICAS:
1. Lee TODA la transcripci√≥n antes de decidir
2. Identifica TODOS los momentos que har√≠an que alguien pause el scroll
3. Ajusta la duraci√≥n EXACTA necesaria para cada momento
4. Incluye contexto suficiente para entender completamente el momento
5. Los tiempos deben ser EXACTOS del video completo
6. No te autolimites - encuentra todos los clips valiosos

FORMATO DE RESPUESTA (JSON ESTRICTO):
{{
    "highlights": [
        {{
            "segment_index": 0,
            "score": 0.85,
            "reason": "Reacci√≥n explosiva inesperada que genera engagement inmediato",
            "start_time": 125.5,
            "end_time": 165.2,
            "optimal_duration": 39.7,
            "viral_category": "emotional_reaction",
            "engagement_prediction": "high_share_potential",
            "duration_rationale": "Duraci√≥n ajustada para capturar toda la reacci√≥n y contexto"
        }}
    ]
}}

NOTA CR√çTICA: NO te limites por n√∫meros artificiales. Si encuentras 8 momentos virales, devuelve 8 clips. Si encuentras 15, devuelve 15. La duraci√≥n debe ser la √ìPTIMA para cada momento espec√≠fico."""
            
            # Hacer llamada a OpenRouter
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/your-repo",
                "X-Title": "Reelify IA Video Analyzer"
            }
            
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 2000
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"]
                        
                        logger.info(f"Respuesta de Deepseek recibida: {content[:200]}...")

                        # Parsear la respuesta JSON robustamente
                        try:
                            json_text = self._extract_json_from_text(content)
                            if json_text is None:
                                raise json.JSONDecodeError('No JSON found', content, 0)
                            analysis_result = json.loads(json_text)
                            highlights = analysis_result.get("highlights", [])

                            logger.info(f"Deepseek parse√≥ {len(highlights)} highlights candidatos")

                            # Calcular duraci√≥n aproximada del video a partir de segmentos (fallback)
                            if segment_transcriptions:
                                video_duration = max(seg.get('end', 0) for seg in segment_transcriptions)
                            else:
                                video_duration = 0.0

                            # Mapear √≠ndices de segmento a tiempos reales
                            mapped_highlights = []
                            for i, highlight in enumerate(highlights):
                                segment_idx = highlight.get("segment_index", 0)
                                if segment_idx < len(segment_transcriptions):
                                    segment = segment_transcriptions[segment_idx]

                                    # Intentar parsear distintos formatos que pueda devolver Deepseek
                                    raw_start = highlight.get("start_time")
                                    raw_end = highlight.get("end_time")
                                    raw_duration = highlight.get("duration") or highlight.get("optimal_duration")

                                    parsed_start = self._parse_time_to_seconds(raw_start)
                                    parsed_end = self._parse_time_to_seconds(raw_end)
                                    parsed_duration = self._parse_time_to_seconds(raw_duration)

                                    # Si ambos tiempos est√°n presentes y v√°lidos, √∫salos
                                    if parsed_start is not None and parsed_end is not None:
                                        final_start = parsed_start
                                        final_end = parsed_end
                                        logger.info(f"Highlight {i+1}: Usando tiempos espec√≠ficos de Deepseek: {final_start:.2f}s - {final_end:.2f}s")
                                    else:
                                        # Si hay s√≥lo duraci√≥n, centrarla en el segmento
                                        if parsed_duration is not None:
                                            center_seg = (segment["start"] + segment["end"]) / 2
                                            final_start = center_seg - parsed_duration / 2
                                            final_end = center_seg + parsed_duration / 2
                                            logger.info(f"Highlight {i+1}: Usando duration proporcionada: {parsed_duration:.1f}s -> {final_start:.2f}s - {final_end:.2f}s")
                                        else:
                                            # Fallback al segmento completo, con intento de ajustar al texto (si Deepseek indica offsets relativos)
                                            final_start = segment["start"]
                                            final_end = segment["end"]
                                            # Intento: si start_time es string tipo '00:01:23' relativo al segmento, convertir sumando
                                            if isinstance(raw_start, str) and ":" in raw_start:
                                                rel = self._parse_time_to_seconds(raw_start)
                                                if rel is not None and rel < (segment["end"] - segment["start"]):
                                                    final_start = segment["start"] + rel
                                            if isinstance(raw_end, str) and ":" in raw_end:
                                                rel = self._parse_time_to_seconds(raw_end)
                                                if rel is not None and rel <= (segment["end"] - segment["start"]):
                                                    final_end = segment["start"] + rel
                                            logger.info(f"Highlight {i+1}: Usando tiempos del segmento como fallback: {final_start:.2f}s - {final_end:.2f}s")

                                    # Clamp dentro del video
                                    final_start = self._clamp(final_start, 0.0, video_duration)
                                    final_end = self._clamp(final_end, 0.0, video_duration)

                                    # Si end <= start, expandir ligeramente alrededor del segmento
                                    if final_end <= final_start:
                                        final_start = max(0.0, segment["start"]) 
                                        final_end = min(video_duration, segment["end"]) 

                                    # Asegurar duraci√≥n m√≠nima
                                    if final_end - final_start < self.absolute_min_duration:
                                        add = (self.absolute_min_duration - (final_end - final_start)) / 2
                                        final_start = max(0.0, final_start - add)
                                        final_end = min(video_duration, final_end + add)

                                    mapped_highlights.append({
                                        "start": float(final_start),
                                        "end": float(final_end),
                                        "score": float(highlight.get("score", 0.5)),
                                        "reason": highlight.get("reason", "Momento destacado identificado por IA"),
                                        "transcription": segment.get("transcription", "")
                                    })

                            # Filtrar clips solapados o muy cercanos
                            filtered_highlights = self._filter_overlapping_clips(mapped_highlights)
                            logger.info(f"Deepseek identific√≥ {len(mapped_highlights)} highlights, filtrados a {len(filtered_highlights)} clips v√°lidos")
                            return filtered_highlights

                        except json.JSONDecodeError as e:
                            logger.error(f"Error parseando respuesta de Deepseek: {e}")
                            logger.error(f"Contenido recibido: {content}")
                            return []
                    else:
                        error_text = await response.text()
                        logger.error(f"Error en API de OpenRouter: {response.status} - {error_text}")
                        return []
        
        except Exception as e:
            logger.error(f"Error en an√°lisis con Deepseek: {e}")
            return []
    
    def _filter_overlapping_clips(self, highlights: List[Dict]) -> List[Dict]:
        """
        Filtrado avanzado de clips con algoritmo de optimizaci√≥n viral.
        Usa programaci√≥n din√°mica para seleccionar la mejor combinaci√≥n.
        """
        if not highlights:
            return []
        
        logger.info(f"Iniciando filtrado avanzado de {len(highlights)} clips candidatos")
        
        # Convertir a ClipCandidates con an√°lisis completo y generar candidatos alternativos
        candidates: List[ClipCandidate] = []
        for highlight in highlights:
            start = float(highlight["start"])
            end = float(highlight["end"])
            transcription = highlight.get("transcription", "") or ""
            base_score = float(highlight.get("score", 0.5))

            # An√°lisis viral avanzado
            viral_analysis = self._analyze_viral_content(transcription)
            emotional_intensity = viral_analysis['score']
            confidence = viral_analysis['confidence']

            # An√°lisis de claridad del habla
            duration = max(0.01, end - start)
            speech_clarity = self._analyze_speech_clarity(transcription, duration)

            # An√°lisis de flujo conversacional
            conversation_flow = self._analyze_conversation_flow(transcription)

            # Keyword density (palabras por segundo)
            keyword_density = len(transcription.split()) / duration if duration > 0 else 0.0

            # Audio energy placeholder (puede estimarse via librosa si se dispone del audio)
            audio_energy = 0.5

            # Crear candidato principal
            candidate = ClipCandidate(
                start=start,
                end=end,
                base_score=base_score,
                emotional_intensity=emotional_intensity,
                speech_clarity=speech_clarity,
                keyword_density=keyword_density,
                conversation_flow=conversation_flow,
                audio_energy=audio_energy,
                final_score=0.0,
                reason=highlight.get("reason", "Momento destacado"),
                transcription=transcription,
                confidence=confidence
            )

            # Calcular score final avanzado
            candidate.final_score = self._calculate_advanced_score(candidate)
            candidates.append(candidate)

            # Generar variantes: usar la heur√≠stica de duration y a√±adir peque√±as variaciones deterministas
            base_target = self._compute_candidate_duration(candidate)
            # Variantes: extendida (contexto), compacta (gancho) y original ajustada
            variant_factors = [1.25, 0.85, 1.0]
            for factor in variant_factors:
                target = max(self.absolute_min_duration, min(self.absolute_max_duration, base_target * factor))
                # determinista peque√±o offset para evitar duraciones id√©nticas
                offset = ((hash((round(start,2), round(target,2), int(factor*100))) % 9) - 4) / 100.0
                target = max(self.absolute_min_duration, target * (1.0 + offset))
                center = (start + end) / 2.0
                s = max(0.0, center - target / 2.0)
                e = s + target
                if e - s >= self.absolute_min_duration:
                    var_candidate = ClipCandidate(
                        start=s,
                        end=e,
                        base_score=base_score * (0.98 if factor==1.0 else 0.95),
                        emotional_intensity=emotional_intensity,
                        speech_clarity=speech_clarity,
                        keyword_density=keyword_density,
                        conversation_flow=conversation_flow,
                        audio_energy=audio_energy,
                        final_score=0.0,
                        reason=f"{highlight.get('reason', 'Momento')} (variante)",
                        transcription=transcription,
                        confidence=confidence
                    )
                    var_candidate.final_score = self._calculate_advanced_score(var_candidate)
                    candidates.append(var_candidate)
        
        # Aplicar algoritmo de selecci√≥n √≥ptima
        optimal_clips = self._select_optimal_clips(candidates)
        
        # Convertir de vuelta a diccionarios
        result_clips = []
        for candidate in optimal_clips:
            result_clips.append({
                "start": candidate.start,
                "end": candidate.end,
                "score": candidate.final_score,
                "reason": f"{candidate.reason} (Score: {candidate.final_score:.3f}, Confianza: {candidate.confidence:.3f})",
                "transcription": getattr(candidate, 'transcription', ''),
                "metadata": {
                    "emotional_intensity": candidate.emotional_intensity,
                    "speech_clarity": candidate.speech_clarity,
                    "conversation_flow": candidate.conversation_flow,
                    "confidence": candidate.confidence
                }
            })
        
        logger.info(f"Filtrado completado: {len(result_clips)} clips seleccionados de {len(candidates)} candidatos")
        return result_clips

    def _select_optimal_clips(self, candidates: List[ClipCandidate]) -> List[ClipCandidate]:
        """
        Selecci√≥n √≥ptima de clips usando algoritmo de programaci√≥n din√°mica
        que maximiza el score total mientras respeta las restricciones temporales.
        """
        if not candidates:
            return []
        
        # Ordenar candidatos por tiempo de inicio
        candidates.sort(key=lambda x: x.start)
        
        # Filtrar candidatos por score m√≠nimo pero permitir mayor n√∫mero bas√°ndonos en cantidad de candidatos
        min_viral_score = settings.viral_score_threshold  # umbral base configurable
        viral_candidates = [c for c in candidates if c.final_score >= min_viral_score]

        logger.info(f"Candidatos virales (score >= {min_viral_score}): {len(viral_candidates)} de {len(candidates)}")

        # Si no hay candidatos que cumplan el umbral, relajar progresivamente
        if not viral_candidates:
            relaxed_thresholds = [0.55, 0.5, 0.45, 0.4, 0.35]
            for threshold in relaxed_thresholds:
                viral_candidates = [c for c in candidates if c.final_score >= threshold]
                if viral_candidates:
                    logger.info(f"Se encontraron candidatos con threshold relajado: {threshold} -> {len(viral_candidates)}")
                    break

        # Si a√∫n no hay ninguno, tomar los N mejores (N mayor para generar m√°s clips)
        if not viral_candidates:
            candidates_sorted = sorted(candidates, key=lambda x: x.final_score, reverse=True)
            max_fallback_clips = min(max(5, int(len(candidates) * 0.5)), len(candidates))
            logger.info(f"Sin candidatos virales, tomando los {max_fallback_clips} mejores clips disponibles (fallback)")
            return candidates_sorted[:max_fallback_clips]
        
        # Aplicar algoritmo de selecci√≥n con restricciones temporales din√°micas
        n = len(viral_candidates)
        if n == 1:
            return viral_candidates

        # Usar l√≠mite din√°mico de clips basado en n√∫mero de candidatos detectados
        # Queremos permitir tantos clips virales como se hayan encontrado, hasta un tope razonable
        dynamic_limit = min(self.max_clips_per_video, max(5, n))
        max_clips_allowed = min(dynamic_limit, n)

        # Crear matriz de compatibilidad temporal
        compatible = [[False] * n for _ in range(n)]
        for i in range(n):
            for j in range(i + 1, n):
                clip1, clip2 = viral_candidates[i], viral_candidates[j]
                # Calcular solapamiento real
                latest_start = max(clip1.start, clip2.start)
                earliest_end = min(clip1.end, clip2.end)
                overlap = max(0.0, earliest_end - latest_start)
                # Solapamiento relativo sobre la duraci√≥n mayor
                max_duration = max(clip1.end - clip1.start, clip2.end - clip2.start, 1e-6)
                overlap_ratio = overlap / max_duration
                # Similitud textual
                sim = self._text_similarity(getattr(clip1, 'transcription', ''), getattr(clip2, 'transcription', ''))
                # Si son muy similares, requerir menos solapamiento permitido, si no, ser m√°s permisivo
                sim_threshold = 0.6
                if sim >= sim_threshold:
                    allowed_overlap = 0.35
                else:
                    allowed_overlap = 0.5
                # Considerar compatibles si el overlap es peque√±o y la separaci√≥n minima se respeta
                separation_ok = (clip2.start - clip1.end) >= self.min_clip_separation or (clip1.start - clip2.end) >= self.min_clip_separation
                if overlap_ratio <= allowed_overlap or separation_ok:
                    compatible[i][j] = compatible[j][i] = True
                else:
                    compatible[i][j] = compatible[j][i] = False
        
        # Selecci√≥n greedy primero: priorizar incluir tantos clips virales como sea posible
        selected_clips = []
        # Ordenar por score desc para intentar tomar los momentos m√°s fuertes primero
        for clip in sorted(viral_candidates, key=lambda x: x.final_score, reverse=True):
            # Verificar que no solape excesivamente con clips ya seleccionados
            overlaps = any(max(0.0, min(c.end, clip.end) - max(c.start, clip.start)) / max(1e-6, max(c.end - c.start, clip.end - clip.start)) > 0.6 for c in selected_clips)
            separation_ok = all(abs(clip.start - c.end) >= self.min_clip_separation and abs(c.start - clip.end) >= self.min_clip_separation for c in selected_clips)
            if not overlaps or separation_ok:
                selected_clips.append(clip)
            if len(selected_clips) >= max_clips_allowed:
                break

        # Si la selecci√≥n greedy no devolvi√≥ suficientes clips y n>1, usar DP para obtener una alternativa
        if len(selected_clips) < max_clips_allowed and n > 1:
            dp_selected = self._dp_optimal_selection(viral_candidates, compatible, max_clips_allowed)
            dp_score = sum(c.final_score for c in dp_selected)
            greedy_score = sum(c.final_score for c in selected_clips)
            if dp_score > greedy_score:
                selected_clips = dp_selected

        logger.info(f"Clips seleccionados: {len(selected_clips)} de {n} candidatos (max allowed {max_clips_allowed})")
        # Ordenar por tiempo para resultado final
        selected_clips.sort(key=lambda x: x.start)
        return selected_clips

    def _dp_optimal_selection(self, candidates: List[ClipCandidate], compatible: List[List[bool]], max_clips: int) -> List[ClipCandidate]:
        """Algoritmo de programaci√≥n din√°mica para selecci√≥n √≥ptima con l√≠mite din√°mico"""
        n = len(candidates)
        if n == 0:
            return []
        
        # Usar l√≠mite din√°mico pero con flexibilidad
        max_clips_dynamic = min(max_clips, n)
        
        # dp[i][k] = (score_m√°ximo, clips_seleccionados) usando hasta el clip i con k clips
        dp = [[(0.0, []) for _ in range(max_clips_dynamic + 1)] for _ in range(n)]

        # Inicializaci√≥n para el primer elemento
        for k in range(max_clips_dynamic + 1):
            if k == 1:
                dp[0][k] = (candidates[0].final_score, [candidates[0]])
            else:
                dp[0][k] = (0.0, [])
        
        # Llenar tabla DP
        for i in range(1, n):
            current_candidate = candidates[i]
            
            for k in range(max_clips_dynamic + 1):
                # Opci√≥n 1: No tomar el clip actual
                dp[i][k] = dp[i-1][k]
                
                # Opci√≥n 2: Tomar el clip actual
                if k > 0:
                    # Tomar clip actual solo
                    if k == 1:
                        if current_candidate.final_score > dp[i][k][0]:
                            dp[i][k] = (current_candidate.final_score, [current_candidate])
                    else:
                        # Buscar el mejor clip compatible anterior considerando diversidad textual
                        best_score = 0.0
                        best_combination = []

                        for j in range(i):
                            if compatible[j][i] and dp[j][k-1][0] > 0:
                                prev_score, prev_list = dp[j][k-1]
                                # Penalizar similitud entre current_candidate y cada clip en prev_list
                                sim_penalty = 0.0
                                unique_tokens = set()
                                for pc in prev_list:
                                    s = self._text_similarity(getattr(pc, 'transcription', ''), getattr(current_candidate, 'transcription', ''))
                                    sim_penalty += s
                                    unique_tokens.update([w.lower() for w in getattr(pc, 'transcription', '').split() if w.strip()])

                                # Calcular diversity bonus proporcional a tokens √∫nicos nuevos
                                current_tokens = set([w.lower() for w in getattr(current_candidate, 'transcription', '').split() if w.strip()])
                                new_tokens = current_tokens - unique_tokens
                                diversity_bonus = min(0.2, len(new_tokens) / max(1, len(current_tokens))) if current_tokens else 0.0

                                combined_score = prev_score + current_candidate.final_score - (sim_penalty * 0.15) + diversity_bonus
                                if combined_score > best_score:
                                    best_score = combined_score
                                    # create a new list to avoid shared references
                                    best_combination = list(prev_list) + [current_candidate]

                        if best_score > dp[i][k][0]:
                            dp[i][k] = (best_score, best_combination)
        
        # Encontrar la mejor soluci√≥n
        best_score = 0.0
        best_clips = []
        
        for i in range(n):
            for k in range(1, max_clips_dynamic + 1):
                if dp[i][k][0] > best_score:
                    best_score = dp[i][k][0]
                    best_clips = dp[i][k][1]
        
        logger.info(f"Algoritmo DP seleccion√≥ {len(best_clips)} clips con score total: {best_score:.3f}")
        return best_clips

    def _text_similarity(self, a: str, b: str) -> float:
        """Simple similitud Jaccard basada en tokens de palabras (0..1)."""
        if not a or not b:
            return 0.0
        sa = set([w.strip('.,!?;:()"\'').lower() for w in a.split() if w.strip()])
        sb = set([w.strip('.,!?;:()"\'').lower() for w in b.split() if w.strip()])
        if not sa or not sb:
            return 0.0
        inter = sa.intersection(sb)
        union = sa.union(sb)
        return float(len(inter) / len(union))
    
    def _validate_viral_potential(self, clips: List[Dict]) -> List[Dict]:
        """
        Validaci√≥n flexible para asegurar que los clips tengan potencial viral.
        """
        if not clips:
            return clips
        
        viral_clips = []
        for clip in clips:
            score = clip.get("score", 0)
            reason = clip.get("reason", "")
            duration = clip["end"] - clip["start"]
            
            # Criterios de validaci√≥n m√°s flexibles
            is_viral_worthy = (
                score >= settings.viral_score_threshold and  # Score m√≠nimo configurable
                self.absolute_min_duration <= duration <= self.absolute_max_duration and  # Duraci√≥n flexible
                len(reason) > 5  # Raz√≥n descriptiva m√≠nima
            )
            
            if is_viral_worthy:
                viral_clips.append(clip)
                logger.info(f"Clip validado como viral: {clip['start']:.1f}s-{clip['end']:.1f}s "
                           f"(duraci√≥n: {duration:.1f}s, score: {score:.2f})")
            else:
                logger.info(f"Clip descartado en validaci√≥n: {clip['start']:.1f}s-{clip['end']:.1f}s "
                           f"(duraci√≥n: {duration:.1f}s, score: {score:.2f}) - "
                           f"Razones: score < {settings.viral_score_threshold}, "
                           f"duraci√≥n fuera de rango [{self.absolute_min_duration}-{self.absolute_max_duration}]")
        
        logger.info(f"Validaci√≥n completada: {len(viral_clips)} clips virales de {len(clips)} candidatos")
        return viral_clips
    
    def _convert_to_clips_with_metadata(self, highlights: List[Dict], video_duration: float) -> List[Dict[str, Any]]:
        """Convierte highlights a clips v√°lidos con metadatos completos y duraci√≥n din√°mica"""
        clips = []
        
        for highlight in highlights:
            start = float(highlight.get("start", 0))
            end = float(highlight.get("end", 0))
            score = highlight.get("score", 0.5)
            reason = highlight.get("reason", "Momento destacado identificado por IA")
            duration = end - start
            
            # Validar que el clip tenga sentido
            if duration <= 0:
                continue
            
            # Usar duraci√≥n din√°mica basada en el an√°lisis de Deepseek
            optimal_duration = highlight.get("optimal_duration")
            if optimal_duration:
                # Si Deepseek especifica una duraci√≥n √≥ptima, usarla
                target_duration = float(optimal_duration)
                logger.info(f"Usando duraci√≥n √≥ptima de Deepseek: {target_duration:.1f}s")
                
                # Ajustar tiempos manteniendo el centro del momento
                center_time = (start + end) / 2
                new_start = max(0, center_time - target_duration / 2)
                new_end = min(video_duration, center_time + target_duration / 2)
                
                # Verificar que no exceda los l√≠mites del video
                if new_end - new_start < target_duration:
                    # Ajustar hacia atr√°s si es necesario
                    if new_end == video_duration:
                        new_start = max(0, video_duration - target_duration)
                    else:
                        new_end = min(video_duration, new_start + target_duration)
                
                start = new_start
                end = new_end
                duration = end - start
                # Aplicar jitter determinista leve a la duraci√≥n para diversidad entre clips
                jitter = (self._deterministic_jitter(int(start*1000)) - 0.5) * 0.08  # +-8%
                duration = max(self.absolute_min_duration, min(self.absolute_max_duration, duration * (1.0 + jitter)))
                # Recalcular start/end manteniendo el centro
                start = max(0, center_time - duration / 2)
                end = min(video_duration, start + duration)
            else:
                # Calcular una duraci√≥n objetivo usando heur√≠stica basada en transcripci√≥n y metadata
                fake_candidate = ClipCandidate(
                    start=start,
                    end=end,
                    base_score=score,
                    emotional_intensity=0.0,
                    speech_clarity=0.0,
                    keyword_density=0.0,
                    conversation_flow=0.0,
                    audio_energy=0.0,
                    final_score=score,
                    reason=reason,
                    transcription=highlight.get('transcription', ''),
                    confidence=0.5
                )

                target_duration = self._compute_candidate_duration(fake_candidate, video_duration=video_duration)
                # Ajustar el clip manteniendo el centro del momento
                center = (start + end) / 2.0
                # Aplicar peque√±as variantes deterministas para generar m√∫ltiples opciones
                base_target = target_duration
                variants = [0.85, 1.0, 1.25]
                chosen_variant = variants[int(abs(hash((start, end))) % len(variants))]
                target_duration = max(self.absolute_min_duration, min(self.absolute_max_duration, base_target * chosen_variant))
                start = max(0, center - target_duration / 2)
                end = min(video_duration, center + target_duration / 2)
                duration = end - start
                logger.info(f"Clip ajustado din√°micamente: {start:.1f}s - {end:.1f}s (duraci√≥n objetivo: {target_duration:.1f}s)")
            
            clip_data = {
                "start": start,
                "end": end,
                "score": score,
                "reason": reason,
                "duration": duration,
                "duration_rationale": highlight.get("duration_rationale", "Duraci√≥n ajustada autom√°ticamente")
            }
            clips.append(clip_data)
            
            logger.info(f"Clip con metadatos din√°micos: {start:.2f}s - {end:.2f}s "
                       f"(duraci√≥n: {duration:.1f}s, score: {score:.2f})")
        
        return clips

    def _convert_to_clips(self, highlights: List[Dict]) -> List[Tuple[float, float]]:
        """Convierte highlights a clips v√°lidos con duraciones apropiadas y din√°micas"""
        clips = []
        
        for highlight in highlights:
            start = float(highlight.get("start", 0))
            end = float(highlight.get("end", 0))
            duration = end - start
            
            # Validar que el clip tenga sentido
            if duration <= 0:
                continue
            
            # Usar duraci√≥n din√°mica basada en an√°lisis de Deepseek
            optimal_duration = highlight.get("optimal_duration")
            if optimal_duration:
                # Si Deepseek especifica una duraci√≥n √≥ptima, respetarla
                target_duration = float(optimal_duration)
                center_time = (start + end) / 2
                start = max(0, center_time - target_duration / 2)
                end = center_time + target_duration / 2
                duration = end - start
                # A√±adir peque√±a variaci√≥n determinista por clip
                jitter = (self._deterministic_jitter(int(start*1000)) - 0.5) * 0.08
                duration = max(self.absolute_min_duration, min(self.absolute_max_duration, duration * (1.0 + jitter)))
                start = max(0, center_time - duration / 2)
                end = min(end, start + duration)
                logger.info(f"Aplicando duraci√≥n √≥ptima de Deepseek: {target_duration:.1f}s (ajustada a {duration:.1f}s)")
            else:
                # Ajustar duraci√≥n si es necesario con l√≠mites absolutos
                if duration < self.absolute_min_duration:
                    # Extender el clip para alcanzar la duraci√≥n m√≠nima
                    extension = (self.absolute_min_duration - duration) / 2
                    start = max(0, start - extension)
                    end = end + extension
                    duration = end - start
                
                if duration > self.absolute_max_duration:
                    # Acortar el clip a la duraci√≥n m√°xima
                    end = start + self.absolute_max_duration
                    duration = self.absolute_max_duration

            # Generar una variante compacta y una extendida si el clip est√° dentro de un rango adecuado
            try:
                center_time = (start + end) / 2
                base_duration = duration
                variants = []
                # Compacta (gancho)
                compact = max(self.absolute_min_duration, base_duration * 0.75)
                # Extendida (contexto)
                extended = min(self.absolute_max_duration, base_duration * 1.4)
                # A√±adir si son diferentes
                if abs(compact - base_duration) / base_duration > 0.12:
                    variants.append((center_time - compact / 2, center_time + compact / 2))
                if abs(extended - base_duration) / base_duration > 0.12:
                    variants.append((center_time - extended / 2, center_time + extended / 2))
                # Insertar variantes deterministas antes del clip principal para aumentar n√∫mero de salidas
                for vs, ve in variants:
                    vs_clamped = max(0, vs)
                    ve_clamped = min(ve, ve if ve <= (vs + self.absolute_max_duration) else vs + self.absolute_max_duration)
                    if ve_clamped - vs_clamped >= self.absolute_min_duration:
                        clips.append({
                            "start": vs_clamped,
                            "end": ve_clamped,
                            "score": highlight.get("score", 0.5),
                            "reason": f"Variante - {highlight.get('reason', '')}"
                        })
            except Exception:
                pass
            
            clips.append({
                "start": start,
                "end": end,
                "score": highlight.get("score", 0.5),
                "reason": highlight.get("reason", "Momento destacado identificado por IA")
            })
            
            logger.info(f"Clip identificado din√°mico: {start:.2f}s - {end:.2f}s "
                       f"(duraci√≥n: {duration:.1f}s, score: {highlight.get('score', 0):.2f})")
        
        # Aplicar filtro de solapamiento
        filtered_highlights = self._filter_overlapping_clips(clips)
        
        # Convertir a tuplas para mantener compatibilidad
        return [(clip["start"], clip["end"]) for clip in filtered_highlights]
    
    async def _fallback_analysis_with_metadata(self, video_path: str) -> List[Dict[str, Any]]:
        """An√°lisis de respaldo con metadatos cuando no est√° disponible la API"""
        logger.info("Usando an√°lisis de respaldo con metadatos (selecci√≥n inteligente de segmentos)")
        
        duration = await self._get_video_duration(video_path)
        if duration <= 0:
            return []
        
        segments = []
        
        if duration < self.absolute_min_duration:
            return []
        
        if duration <= self.absolute_max_duration:
            return [{
                "start": 0.0,
                "end": duration,
                "score": 0.6,
                "reason": "Video completo - duraci√≥n adecuada",
                "duration": duration
            }]
        
        # Crear clips estrat√©gicamente distribuidos con duraci√≥n din√°mica
        clips_per_hour = 4
        total_clips = min(self.max_clips_per_video, max(2, int(duration / 3600 * clips_per_hour)))

        min_segment_duration = self.optimal_clip_duration[0]
        max_segment_duration = self.optimal_clip_duration[1]

        for i in range(total_clips):
            # Posici√≥n relativa en el video (0..1)
            segment_position = (i + 0.5) / total_clips

            # Calcular duraci√≥n objetivo usando helper inteligente
            segment_duration = self._compute_backup_segment_duration(
                position=segment_position,
                index=i,
                total=total_clips,
                min_d=min_segment_duration,
                max_d=max_segment_duration
            )

            # Centrar el clip en la posici√≥n calculada
            center = segment_position * duration
            start_time = max(0, center - segment_duration / 2)
            end_time = min(duration, start_time + segment_duration)
            # Ajustar start si el end toc√≥ el final
            start_time = max(0, end_time - segment_duration)

            if end_time - start_time >= self.absolute_min_duration:
                actual_duration = end_time - start_time
                segments.append({
                    "start": start_time,
                    "end": end_time,
                    "score": 0.5 + (i * 0.05),
                    "reason": f"Segmento estrat√©gico {i + 1} - selecci√≥n autom√°tica distribuida (duraci√≥n: {actual_duration:.1f}s)",
                    "duration": actual_duration
                })

                logger.info(f"Segmento de respaldo {i+1}: {start_time:.1f}s - {end_time:.1f}s (duraci√≥n: {actual_duration:.1f}s)")
        
        return segments

    async def _fallback_analysis(self, video_path: str) -> List[Tuple[float, float]]:
        """An√°lisis de respaldo cuando no est√° disponible la API"""
        logger.info("Usando an√°lisis de respaldo (selecci√≥n inteligente de segmentos)")
        
        duration = await self._get_video_duration(video_path)
        if duration <= 0:
            return []
        
        segments = []
        
        if duration < self.absolute_min_duration:
            return []
        
        if duration <= self.absolute_max_duration:
            return [(0.0, duration)]
        
        clips_per_hour = 4
        total_clips = min(self.max_clips_per_video, max(2, int(duration / 3600 * clips_per_hour)))

        min_segment_duration = self.optimal_clip_duration[0]
        max_segment_duration = self.optimal_clip_duration[1]

        for i in range(total_clips):
            segment_position = (i + 0.5) / total_clips

            segment_duration = self._compute_backup_segment_duration(
                position=segment_position,
                index=i,
                total=total_clips,
                min_d=min_segment_duration,
                max_d=max_segment_duration
            )

            center = segment_position * duration
            start_time = max(0, center - segment_duration / 2)
            end_time = min(duration, start_time + segment_duration)
            start_time = max(0, end_time - segment_duration)

            if end_time - start_time >= self.absolute_min_duration:
                segments.append((start_time, end_time))
                logger.info(f"Segmento de respaldo {i+1}: {start_time:.1f}s - {end_time:.1f}s "
                           f"(duraci√≥n: {end_time - start_time:.1f}s)")

        
        return segments
    
    async def _get_video_duration(self, video_path: str) -> float:
        """Obtiene la duraci√≥n del video usando FFprobe"""
        try:
            cmd = [
                'ffprobe', 
                '-v', 'quiet',
                '-show_entries', 'format=duration',
                '-of', 'csv=p=0',
                video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                duration = float(result.stdout.strip())
                return duration
            else:
                logger.error(f"FFprobe error: {result.stderr}")
                return 0.0
                
        except Exception as e:
            logger.error(f"Error obteniendo duraci√≥n del video: {e}")
            return 0.0
